{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9079bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022798a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.graders import PythonGraderParam, ScoreModelGrader, MultiGraderParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = pathlib.Path().resolve()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Locate shared root (the folder that contains both `utils` and your project)\n",
    "# It climbs up until it finds a `utils/` directory or stops at filesystem root.\n",
    "# ---------------------------------------------------------------------------\n",
    "ROOT = HERE\n",
    "while ROOT != ROOT.parent and not (ROOT / \"utils\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "if not (ROOT / \"utils\").exists():\n",
    "    raise RuntimeError(\n",
    "        f\"Could not find 'utils' directory above {HERE}. \"\n",
    "        \"Check your project structure or adjust the path resolution logic.\"\n",
    "    )\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "print(f\"✅ Added to sys.path: {ROOT}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Infer project name (parent of pipelines/ or notebooks/)\n",
    "# e.g., .../projects/<project>/notebooks/... -> <project>\n",
    "# ---------------------------------------------------------------------------\n",
    "project_name = HERE.parent.name\n",
    "os.environ.setdefault(\"PROJECT\", project_name)\n",
    "print(f\"✅ Project name set to: {project_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Load project-specific environment variables\n",
    "# ---------------------------------------------------------------------------\n",
    "env_path = HERE.parent / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path, override=True)\n",
    "    print(f\"✅ Loaded .env from: {env_path}\")\n",
    "else:\n",
    "    print(\"⚠️ No .env file found, relying on existing environment variables.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ensure the OpenAI API key is available\n",
    "# ---------------------------------------------------------------------------\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found. Add it to your .env file or export it before running.\"\n",
    "    )\n",
    "print(\"✅ OpenAI API key detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Project Imports (now should work everywhere)\n",
    "# ---------------------------------------------------------------------------\n",
    "from utils import (\n",
    "    infer_item_schema,\n",
    "    build_data_source,\n",
    "    wait_until_finished,\n",
    "    fetch_all_output_items,\n",
    "    extract_items,\n",
    "    save_run,\n",
    "    save_grader,\n",
    "    RunRecord,\n",
    "    load_prompt,\n",
    "    get_or_upload_file,\n",
    ")\n",
    "from utils.project_paths import datasets_root, project_root\n",
    "from utils.plot_eval_runs import (\n",
    "    load_scores_by_item,\n",
    "    compute_score_stats,\n",
    "    plot_score_stats,\n",
    "    plot_score_stats_plotly,\n",
    ")\n",
    "\n",
    "# Ensure structured_outputs can be imported\n",
    "_cust_root = project_root()\n",
    "if str(_cust_root) not in sys.path:\n",
    "    sys.path.append(str(_cust_root))\n",
    "\n",
    "\n",
    "print(\"✅ Utils imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataset auto-discovery: pick first *_{SPLIT}.jsonl file under `data/`\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Select data split ---------------------------------------------------------\n",
    "SPLIT = \"train\"  # choose \"train\", \"val\", \"test\" etc.\n",
    "\n",
    "try:\n",
    "    DATA_PATH = next(datasets_root().glob(f\"*_{SPLIT}.jsonl\"))\n",
    "except StopIteration as e:\n",
    "    raise FileNotFoundError(\"No *_train.jsonl dataset found in data/ folder\") from e\n",
    "\n",
    "DATASET_NAME = project_name  # use folder name as dataset identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response JSON schema -------------------------------------------------------\n",
    "from openai.lib._pydantic import to_strict_json_schema \n",
    "from structured_outputs.base_models import Level1Codes\n",
    "\n",
    "schema = to_strict_json_schema(Level1Codes)\n",
    "RESPONSE_FORMAT: Dict[str, Any] = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"name\": Level1Codes.__name__,\n",
    "    \"schema\": schema,\n",
    "    \"strict\": True,\n",
    "}\n",
    "example = Level1Codes(level1=[\n",
    "    {\"code\": \"environment\"},\n",
    "    {\"code\": \"agriculture, forestry and fisheries\"}\n",
    "])\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Example Level1Codes output:\\n\" + \"-\"*40)\n",
    "pprint(example.model_dump(), sort_dicts=False)\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grader --------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "# Precision, Recall, and F1 graders (F1 both as Python & Multi)\n",
    "\n",
    "# -- Grader source strings --------------------------------------------------\n",
    "\n",
    "precision_grader_source = \"\"\"\n",
    "def grade(sample, item) -> float:\n",
    "    pred_list = (sample or {}).get(\"output_json\", {}).get(\"level1\", [])\n",
    "    if not isinstance(pred_list, list):\n",
    "        return 0.0\n",
    "    pred_set = {d.get(\"code\") for d in pred_list if isinstance(d, dict) and d.get(\"code\")}\n",
    "\n",
    "    ref_set = set(item[\"reference_answer\"])\n",
    "    if not pred_set:\n",
    "        return 0.0\n",
    "    inter = len(pred_set & ref_set)\n",
    "    return inter / len(pred_set)\n",
    "\"\"\"\n",
    "\n",
    "recall_grader_source = \"\"\"\n",
    "def grade(sample, item) -> float:\n",
    "    pred_list = (sample or {}).get(\"output_json\", {}).get(\"level1\", [])\n",
    "    if not isinstance(pred_list, list):\n",
    "        return 0.0\n",
    "    pred_set = {d.get(\"code\") for d in pred_list if isinstance(d, dict) and d.get(\"code\")}\n",
    "\n",
    "    ref_set = set(item[\"reference_answer\"])\n",
    "    if not ref_set:\n",
    "        return 0.0\n",
    "    inter = len(pred_set & ref_set)\n",
    "    return inter / len(ref_set)\n",
    "\"\"\"\n",
    "\n",
    "f1_grader_source = \"\"\"\n",
    "def grade(sample, item) -> float:\n",
    "    output = sample.get(\"output_json\")\n",
    "    if not output or not isinstance(output, dict):\n",
    "        return 0.0\n",
    "\n",
    "    pred_list = output.get(\"level1\", [])\n",
    "    if not isinstance(pred_list, list):\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        pred_set = {d[\"code\"] for d in pred_list if isinstance(d, dict) and \"code\" in d}\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "    ref_set = set(item[\"reference_answer\"])\n",
    "    if not pred_set or not ref_set:\n",
    "        return 0.0\n",
    "\n",
    "    inter = len(pred_set & ref_set)\n",
    "    precision = inter / len(pred_set)\n",
    "    recall = inter / len(ref_set)\n",
    "    denom = precision + recall\n",
    "    return (2 * precision * recall / denom) if denom else 0.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def294ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Instantiate graders ----------------------------------------------------\n",
    "\n",
    "precision_grader = PythonGraderParam(\n",
    "    type=\"python\",\n",
    "    name=\"level1_precision\",\n",
    "    source=precision_grader_source,\n",
    "    image_tag=\"2025-05-08\",\n",
    "    pass_threshold=0.8\n",
    ")\n",
    "\n",
    "recall_grader = PythonGraderParam(\n",
    "    type=\"python\",\n",
    "    name=\"level1_recall\",\n",
    "    source=recall_grader_source,\n",
    "    image_tag=\"2025-05-08\",\n",
    "    pass_threshold=0.8\n",
    ")\n",
    "\n",
    "f1_python_grader = PythonGraderParam(\n",
    "    type=\"python\",\n",
    "    name=\"level1_f1_python\",\n",
    "    source=f1_grader_source,\n",
    "    image_tag=\"2025-05-08\",\n",
    "    pass_threshold=0.8\n",
    ")\n",
    "\n",
    "# MultiGrader uses max() to avoid division by zero\n",
    "f1_multi_grader = MultiGraderParam(\n",
    "    type=\"multi\",\n",
    "    name=\"level1_f1_multi\",\n",
    "    graders={\n",
    "        \"precision\": precision_grader,\n",
    "        \"recall\": recall_grader,\n",
    "    },\n",
    "    calculate_output=\"2 * precision * recall / max(precision + recall, 1e-9)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d064c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist grader definitions\n",
    "for g in (precision_grader, recall_grader, f1_python_grader, f1_multi_grader):\n",
    "    print('Saving', g[\"name\"])\n",
    "    save_grader(g)\n",
    "\n",
    "# Collect all graders for the eval – keep F1 multi first so it appears as the primary result\n",
    "GRADERS = [f1_python_grader, precision_grader, recall_grader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT_NAME = \"v7\"\n",
    "prompt_obj = load_prompt(DATASET_NAME, PROMPT_NAME, prompt_type=\"developer\")\n",
    "if prompt_obj is None:\n",
    "    raise RuntimeError(f\"Prompt {PROMPT_NAME} not found – create it under prompts/{DATASET_NAME}/\")\n",
    "prompt = prompt_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b478ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset & create eval\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key,\n",
    "    project=os.getenv(\"OPENAI_PROJECT_ID\"),\n",
    ")\n",
    "file_id = await get_or_upload_file(client, DATA_PATH)\n",
    "item_schema = infer_item_schema(DATA_PATH)\n",
    "\n",
    "eval_obj = await client.evals.create(\n",
    "    name=f\"law-codes-{prompt.name}\",\n",
    "    metadata={\"description\": f\"Live eval – {prompt.name}\"},\n",
    "    data_source_config={\n",
    "        \"type\": \"custom\",\n",
    "        \"item_schema\": item_schema,\n",
    "        \"include_sample_schema\": True,\n",
    "    },\n",
    "    testing_criteria=GRADERS,\n",
    ")\n",
    "eval_id = eval_obj.id\n",
    "print(\"Eval created:\", eval_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"o4-mini\"\n",
    "MODEL_PARAMS: Dict[str, Any] = {\n",
    "    # Standard response params\n",
    "    \"seed\": 42,\n",
    "    \"temperature\": None,\n",
    "    \"top_p\": None,\n",
    "    \"max_completions_tokens\": None,\n",
    "    \"text\": {\"format\": RESPONSE_FORMAT},  # or None to disable JSON mode\n",
    "    # Reasoning-specific params (responses models)\n",
    "    \"reasoning_effort\": \"low\", #\"medium\",  # set to None or \"low\"/\"medium\"/\"high\"\n",
    "    # Tools / function calling\n",
    "    \"tools\": None,\n",
    "}\n",
    "# Remove keys with explicit None so we don't send them to the API\n",
    "MODEL_PARAMS = {k: v for k, v in MODEL_PARAMS.items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data_source config\n",
    "USER_FIELD = \"text_input\"\n",
    "data_source = build_data_source(\n",
    "    prompt,\n",
    "    file_id,\n",
    "    USER_FIELD,\n",
    "    model=MODEL_NAME,\n",
    "    model_params=MODEL_PARAMS,\n",
    "    datasource_type=\"responses\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de981c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run loop\n",
    "N_RUNS = 1\n",
    "for i in range(N_RUNS):\n",
    "    print(f\"\\n=== Run {i + 1}/{N_RUNS} ===\")\n",
    "    run = await client.evals.runs.create(\n",
    "        eval_id=eval_id,\n",
    "        name=f\"variance-{prompt.name}-run{i + 1}\",\n",
    "        data_source=data_source,\n",
    "    )\n",
    "    print(\"Run URL:\", getattr(run, \"report_url\", \"<no url>\"))\n",
    "\n",
    "    run_fin = await wait_until_finished(client, eval_id, run.id)\n",
    "    if run_fin.status == \"failed\":\n",
    "        print(\"Run failed. Error details:\")\n",
    "        print(getattr(run_fin, \"error\", \"<no error field>\"))\n",
    "        print(getattr(run_fin, \"error_message\", \"<no error_message field>\"))\n",
    "        continue\n",
    "    # If run succeeded, proceed to fetch outputs\n",
    "    items_raw = await fetch_all_output_items(client, eval_id, run.id)\n",
    "    items = extract_items(items_raw)\n",
    "\n",
    "    # Quick peek ---------------------------------------------------------\n",
    "    if items_raw:\n",
    "        content_preview = items_raw[0].sample.output[0].content if items_raw[0].sample and items_raw[0].sample.output else \"<no output>\"\n",
    "        print(\"First assistant output:\", content_preview)\n",
    "\n",
    "    # Save run metadata once with all graders\n",
    "    record = RunRecord(\n",
    "        dataset=DATASET_NAME,\n",
    "        prompt=vars(prompt),\n",
    "        eval_id=eval_id,\n",
    "        run_id=run.id,\n",
    "        model=MODEL_NAME,\n",
    "        grader_name=None,  # deprecated – use grader_names instead\n",
    "        grader_names=[g[\"name\"] if isinstance(g, dict) else getattr(g, \"name\", str(g)) for g in GRADERS],\n",
    "        timestamp=datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
    "        reasoning_effort=MODEL_PARAMS.get(\"reasoning_effort\", None),\n",
    "        split=SPLIT,\n",
    "        items=items,\n",
    "    )\n",
    "    save_run(record)\n",
    "\n",
    "    # Accuracy per-grader using the new score dict structure\n",
    "    for grader in GRADERS:\n",
    "        scores: List[float] = [\n",
    "            it[\"score\"].get(grader[\"name\"] if isinstance(grader, dict) else getattr(grader, \"name\", None))\n",
    "            for it in items\n",
    "            if it[\"score\"].get(grader[\"name\"] if isinstance(grader, dict) else getattr(grader, \"name\", None)) is not None\n",
    "        ]\n",
    "        if scores:\n",
    "            gname = grader[\"name\"] if isinstance(grader, dict) else getattr(grader, \"name\", \"<unknown>\")\n",
    "            print(f\"{gname} Accuracy = {sum(scores)/len(scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f000e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "for grader in GRADERS:\n",
    "    scores_by_item, runs = load_scores_by_item(\n",
    "        DATASET_NAME,\n",
    "        prompt_id=prompt.id,\n",
    "        model=MODEL_NAME,\n",
    "        grader_name=grader[\"name\"] if isinstance(grader, dict) else getattr(grader, \"name\", None),\n",
    "        reasoning_effort=MODEL_PARAMS.get(\"reasoning_effort\", None),\n",
    "        split=SPLIT,\n",
    "    )\n",
    "\n",
    "    stats = compute_score_stats(scores_by_item)\n",
    "    gname = grader[\"name\"] if isinstance(grader, dict) else getattr(grader, \"name\", \"<unknown>\")\n",
    "    print(f\"Plotting results for {gname} – {len(runs)} runs\")\n",
    "    plot_score_stats_plotly(stats, n_runs=len(runs), context=gname, width=1000, height=600, dark_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62ebbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
